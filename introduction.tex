
\section{Introduction and History}

GPUs with programmable shader supporting floating point operations were the beginning of GPGPU.
It started by using graphics libraries to implement tensor operations.
One of the first implementations was a LU factorization in 2005 \cite{du2012cuda}.
Nvidia's CUDA platform was launched in 2008 to allow programming while ignoring the underlying graphical concepts.
Not alike alternatives such as OpenCL, CUDA only works on Nvidia GPUs.
It has been linked since the beginning with their Tesla family of GPUs.
Just as GeForce is their Desktop family, and Jetson their embedded family, Tesla is their HPC family.
Tesla was also the name of their microarchitecture in 2008, when they introduced CUDA.
Since then there has been a mostly continuous progress towards their latest microarchitecture, Pascal, presented in April 2016.
The biggest changes were introduced in the Fermi microarchitecture.
% This recency makes that to date there is no technical analysis of the new microarchitecture, as there is of previous ones.
% The main source of information comes from the Whitepaper for the GP100 GPU, presenting the Pascal microarchitecture.

In 2012 Nvidia presented the Fermi microarchitecture, bridging many of the gaps needed for intensive use in HPC.
Fermi introduced 40-bit memory addressing, and ISA support for up to 64-bit addressing \cite{nickolls2010gpu}.
It adopted the IEE 754-2008 floating point standard and enabled 64-bit floating point operations at only half of the thoughput of FP32.
Fermi also introduced the current hierarchical cache with L1 cache connected to L2 cache and L2 to DRAM, with unified access.
L2 and DRAM were enabled to access host CPU memory through PCIe.
The amount of L1 and L2 were configurable for the programmer, allowing a variable fixed fraction of shared memory.

The next generation, Kepler, was introduced in 2012 and introduced Dynamic Parallelism.
Dynamic Parallelism refers to enabling GPU threads to launch other threads (kernels launching kenerls).
This provides the programming model with more flexibility.
This architecture is the one currently in use for GPGPU HPC tasks requiring 64-bit floating point precission.

The Maxwell microarchitecture, presented in 2014 introduced Unified Virtual Memory.
This is an important step in simplifying the programming model.
In this generation FP64 efficiency dropped by one order of magnitude.
It focused on graphics and per Watt performance.

The Pascal microarchitecture, presented in 2016, brings the features of the Kepler and Maxwell microarchitectures together.
It improves the Unified Virtual Memory model, thanks to page faulting and a new GPU-to-GPU and GPU-to-CPU interconnect NVLink.
It is an alternative to PCIe, but it does not replace it.
It also brings back FP64 at high rates.
Memory bandwidth is improved thanks to the use of HBM2 memory.
FP16 operations are supported at double the rate of FP32 operations.
Pascal also introduces preemption at the instruction level.
All this comes with a reduction of the transistor size by using a 16nm FinFET technology.

%
%
%
% \subsection{Tesla Family}
% Architectures:
% \begin{itemize}
%     \item Tesla (2008)
%         \begin{itemize}
%             \item CUDA
%         \end{itemize}
%     \item Fermi (2010)
%         \begin{itemize}
%           \item 40-bit memory addressing. ISA support for up to 64-bit.
%           \item Introduces hierarchical cache. L1 is connected to L2 and L2 to DRAM.
%           L2 and DRAM have access to host CPU memory through PCIe.
%           \item ECC memory protection enables high MTBF for HPC.
%           \item 64 bit Floating Point operations (only half of the throughput??? \cite{nickolls2010gpu} )
%           \item IEE 754-2008 (new?)
%           \item Unified access to local, shared and global memory (C/C++ compatible)
%           \item Configurable L1 and L2 (shared) memory
%         \end{itemize}
%     \item Kepler (2012)
%         \begin{itemize}
%             \item K80 $\rightarrow$ Multi-Application HPC
%             \item Dynamic Parallelism [\textbf{TODO:} Streams?]
%         \end{itemize}
%     \item Maxwell (2014)
%         \begin{itemize}
%             \item M40 (training) M4 (inference) $\rightarrow$ Hyperscale HPC
%             \item Unified Virtual Memory ?
%             \item DX12 ?
%         \end{itemize}
%     \item Pascal (2016)
%         \begin{itemize}
%             \item P100 $\rightarrow$ Strong-Scale HPC
%             \begin{itemize}
%                 \item Unified Virtual Memory
%                 \item 3D chip. 3D stacked DRAM $\rightarrow$ terabyte bandwidth
%                 \item NVLink 160GB/s (5x bi-directional PCI Express)
%                 \begin{itemize}
%                     \item Keep whole layer weights in registers (see Greg Diamos, there is a talk too)
%                     \item Model paralellism (over NVLink)
%                 \end{itemize}
%                 \item Preemption (all GPU cores) (deep learning?)
%                 \item Huge chip 600mm2?
%                 \item HBM2 fastest memory 4000 wires from Pascal to others around (Samsung?)
%                 \item 16nm FinFET (TSMC)
%             \end{itemize}
%             \item DGX-1 $\rightarrow$ Supercomputer ``in a box'' with 170 FP16 TFLOPS
%             \item Drive PX
%         \end{itemize}
%     \item Volta (2018)
%
% \end{itemize}
%
%
% % can use a bibliography generated by BibTeX as a .bbl file
% % standard IEEE bibliography style from:
% % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/IEEEtran/bibtex
% \bibliographystyle{IEEEtran}
% % argument is your BibTeX string definitions and bibliography database(s)
% \bibliography{IEEEabrv,references}
