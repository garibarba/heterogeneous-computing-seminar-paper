
\section{GPGPU} \label{sec:gpgpu}

% \subsection{A closer look at GPGPU \cite{Hu:2016:CLG:2891449.2873053}}

The paper \cite{Hu:2016:CLG:2891449.2873053} introduces a formal model trying to generalize the computational model of GPUs.
It refers to \cite{nickolls2010gpu} as introducing a new computing era, called GPGPU.
There are many attempts of describing the internal mechanisms of GPU operation.
Some are focused on a graphics perspective, some others on general purpose.
New generation devices are surpassing these models and \cite{Hu:2016:CLG:2891449.2873053} tries to develop a broader model.
The task is difficult since most reports are either too simple or black-box descriptions \cite{Hu:2016:CLG:2891449.2873053}.
White papers focus on capabilities, and programming guides focus on code syntax.
Some micro-architecture descriptions are too focused on a detail, ignoring the rest.

A hierarchical descriptive model for GPGPU is proposed in \cite{Hu:2016:CLG:2891449.2873053}.
It combines the ideas of OpenCL and the Multi-BSP \cite{valiant2008bridging} bridging model.
A Processing Element of a higher level in the hierarchy sees lower-level Processing Elements as atomic.
The CUDA architecture is then described within this model as a two-level structure with nested components.
The architecture details are not public and won't probably ever be \cite{voicu:fermi}.
Thus the description has to be based on official patents and research papers.

\subsubsection{CUDA under the descriptive model}
The GPU is the atomic element forming the second level of the hierarchy.
The first level is composed by the GigaThread Engine, CUDA Work Distributor, Data Transfer Engine, Streaming Multiprocessor array, and local/global memory.
Each Streaming Multiprocessor is broken into analogous components of the 0th level.
These are the Thread controller, Warp scheduler, Load/Store Units, local/shared registers and ALU arrays.

Analogous components of different levels have different behaviours.
For example, the Processing Elements which are the ALU arrays (0th level) and Streaming Multiprocessor array (first level).
Each Streaming Multiprocessor works asynchronously, whereas groups of 32 ALUs have to work synchronously under a common program counter.
Other analogous components at different levels are Load/Store Units vs Data Transfer Engine arrays, local/shared registers vs local/global memory,
task Buffers (program/block counters vs reference counter), Warp scheduler vs CUDA Work Distributor, and Thread Controller vs GigaThread Engine.
The GigaThread work scheduler balances work among Streaming Multiprocessors.
The Streaming Multiprocessors scheduler executes concurrent threads to help reduce the effect of long latency loads from DRAM memory.

\subsubsection{Future Trends proposed in \cite{Hu:2016:CLG:2891449.2873053}}

The current hierarchy consists of 2 levels, but this is not intrinsic to the model.
We could expect to see hierarchies with more levels or tasks of different levels being launched on the same hardware level.
Recursive kernel invocation and maybe the DGX-1 --- with several GPUs forming a higher level --- point in this direction.

The hierarchical cache system reflects the convergence trend between CPU and GPU.
GPGPU is fundamentally a hierarchical system of moving data to computing.
It could make sense to develop a system where computing is moved to data.
We could thus also expect GPUs to part away from this Von Neumann hierarchical model and go in the way of neuromorphic chips.
These neuromorphic chips have ALUs and memory mixed, which allows simultaneous and asynchronous execution of different dataflows.
